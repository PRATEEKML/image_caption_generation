{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_YfLwIOEc0z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### directory to save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2j5XAFXExvS"
   },
   "outputs": [],
   "source": [
    "mw='./mw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXIPtkGwVhRk"
   },
   "outputs": [],
   "source": [
    "os.mkdir(mw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading preprocessed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byFTMtHpFL8K"
   },
   "outputs": [],
   "source": [
    "with open('train_description.pkl','rb') as f:\n",
    "    train_description=pickle.load(f)\n",
    "with open('val_description.pkl','rb') as f:\n",
    "    val_description=pickle.load(f)\n",
    "with open('word_to_index.pkl','rb') as f:\n",
    "    word_to_index=pickle.load(f)\n",
    "with open('index_to_word.pkl','rb') as f:\n",
    "    index_to_word=pickle.load(f)\n",
    "with open('encoding_train.pkl','rb') as f:\n",
    "    encoding_train=pickle.load(f)\n",
    "with open('encoding_val.pkl','rb') as f:\n",
    "    encoding_val=pickle.load(f)\n",
    "embedding_idx=np.load('embedding_idx.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating size of vocabulary and sentence of max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Sz3gx_QdHgTr",
    "outputId": "aece16ea-9874-41ce-9149-ffa0ffd0ffce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1848"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=len(word_to_index)+1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "hmjEwO6iHrp6",
    "outputId": "4178f60b-e342-410d-886d-11520d78a9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ['startseq', 'an', 'african', 'american', 'man', 'wearing', 'green', 'sweatshirt', 'and', 'blue', 'vest', 'is', 'holding', 'up', 'in', 'front', 'of', 'his', 'face', 'while', 'standing', 'on', 'busy', 'sidewalk', 'in', 'front', 'of', 'group', 'of', 'men', 'playing', 'instruments', 'endseq']\n"
     ]
    }
   ],
   "source": [
    "max_len=0\n",
    "for ll in train_description.values():\n",
    "    for caption in ll:\n",
    "        a=caption.split()\n",
    "        if(len(a)>max_len):\n",
    "            max_len=len(a)\n",
    "            b=a\n",
    "print(max_len,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PB1YkedFIqPe"
   },
   "outputs": [],
   "source": [
    "img_input=Input((2048,))\n",
    "drop_img=Dropout(0.5)(img_input)\n",
    "img_act=Dense(256,activation='relu')(drop_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spWgIWDxKVvc"
   },
   "outputs": [],
   "source": [
    "text_input=Input((max_len,))\n",
    "emb=Embedding(vocab_size,50,mask_zero=True,weights=[embedding_idx],trainable=False)(text_input)\n",
    "drog_txt=Dropout(0.5)(emb)\n",
    "lstm=LSTM(256)(drog_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "rf_UZAZTMT5J",
    "outputId": "e74cf992-0385-4759-9390-3ab06b563552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 33)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 33, 50)       92400       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 33, 50)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          314368      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1848)         474936      dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,472,040\n",
      "Trainable params: 1,379,640\n",
      "Non-trainable params: 92,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "combination=Add()([img_act,lstm])\n",
    "dense_1=Dense(256,activation='relu')(combination)\n",
    "dense_2=Dense(vocab_size,activation='softmax')(dense_1)\n",
    "model=Model(inputs=[img_input,text_input],outputs=dense_2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AX44Xd_QUPDy"
   },
   "outputs": [],
   "source": [
    "#train_description-> img_id:sents\n",
    "#word_to_index-> word:index\n",
    "#encoding_train-> img_id:(2048,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozsGykR2NkC3"
   },
   "outputs": [],
   "source": [
    "def data_generator(train_description,word_to_index,encoding_train,batch_size=3,max_len=35):\n",
    "    x1,x2,y=[],[],[]\n",
    "    n=0\n",
    "    while True:\n",
    "        for key,desc_list in train_description.items():\n",
    "            n+=1\n",
    "            photo=encoding_train[key]\n",
    "            for desc in desc_list:\n",
    "                desc=[word_to_index[words] for words in desc.split() if words in word_to_index]\n",
    "                for i in range(1,len(desc)):\n",
    "                    xi=desc[:i]\n",
    "                    yi=desc[i]\n",
    "                    xi=pad_sequences([xi],maxlen=max_len,padding='post',truncating='post')[0]\n",
    "                    yi=to_categorical([yi],num_classes=vocab_size)[0]\n",
    "                    x1.append(photo)\n",
    "                    x2.append(xi)\n",
    "                    y.append(yi)\n",
    "            if(n==batch_size):\n",
    "                x1=np.array(x1)\n",
    "                x2=np.array(x2)\n",
    "                y=np.array(y)\n",
    "                yield([x1,x2],y)\n",
    "                x1,x2,y=[],[],[]\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oixqBHD9rhYI"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8XjlSJ3f13A"
   },
   "outputs": [],
   "source": [
    "epochs=30\n",
    "batch_size=3\n",
    "steps=len(train_description)//batch_size\n",
    "val_steps=len(val_description)//batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training for first 30 epochs at fast learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pCp7p2VQrZeM",
    "outputId": "25a1d053-b0bf-4508-9609-ff2db856d6bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1/30\n",
      "2000/2000 [==============================] - 27s 14ms/step - loss: 3.9663 - val_loss: 3.5815\n",
      "epochs 2/30\n",
      "2000/2000 [==============================] - 26s 13ms/step - loss: 3.4556 - val_loss: 3.4915\n",
      "epochs 3/30\n",
      "2000/2000 [==============================] - 26s 13ms/step - loss: 3.3045 - val_loss: 3.4576\n",
      "epochs 4/30\n",
      "2000/2000 [==============================] - 26s 13ms/step - loss: 3.2046 - val_loss: 3.4571\n",
      "epochs 5/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 3.1323 - val_loss: 3.4752\n",
      "epochs 6/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 3.0745 - val_loss: 3.4941\n",
      "epochs 7/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 3.0247 - val_loss: 3.5028\n",
      "epochs 8/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.9848 - val_loss: 3.5437\n",
      "epochs 9/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.9518 - val_loss: 3.5696\n",
      "epochs 10/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.9210 - val_loss: 3.6037\n",
      "epochs 11/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.8953 - val_loss: 3.6265\n",
      "epochs 12/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.8727 - val_loss: 3.6587\n",
      "epochs 13/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.8511 - val_loss: 3.6837\n",
      "epochs 14/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.8316 - val_loss: 3.7084\n",
      "epochs 15/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.8169 - val_loss: 3.7480\n",
      "epochs 16/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.8019 - val_loss: 3.7714\n",
      "epochs 17/30\n",
      "2000/2000 [==============================] - 27s 14ms/step - loss: 2.7859 - val_loss: 3.7855\n",
      "epochs 18/30\n",
      "2000/2000 [==============================] - 27s 14ms/step - loss: 2.7739 - val_loss: 3.8179\n",
      "epochs 19/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.7623 - val_loss: 3.8439\n",
      "epochs 20/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.7514 - val_loss: 3.8541\n",
      "epochs 21/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.7436 - val_loss: 3.8761\n",
      "epochs 22/30\n",
      "2000/2000 [==============================] - 27s 14ms/step - loss: 2.7336 - val_loss: 3.8691\n",
      "epochs 23/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.7254 - val_loss: 3.8942\n",
      "epochs 24/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.7178 - val_loss: 3.9179\n",
      "epochs 25/30\n",
      "2000/2000 [==============================] - 27s 14ms/step - loss: 2.7083 - val_loss: 3.9389\n",
      "epochs 26/30\n",
      "2000/2000 [==============================] - 27s 14ms/step - loss: 2.7042 - val_loss: 3.9367\n",
      "epochs 27/30\n",
      "2000/2000 [==============================] - 27s 14ms/step - loss: 2.6938 - val_loss: 3.9835\n",
      "epochs 28/30\n",
      "2000/2000 [==============================] - 27s 14ms/step - loss: 2.6871 - val_loss: 4.0054\n",
      "epochs 29/30\n",
      "2000/2000 [==============================] - 27s 14ms/step - loss: 2.6829 - val_loss: 4.0100\n",
      "epochs 30/30\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 2.6768 - val_loss: 4.0173\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    gen=data_generator(train_description,word_to_index,encoding_train,batch_size,max_len)\n",
    "    val_gen=data_generator(val_description,word_to_index,encoding_val,batch_size,max_len)\n",
    "    print('epochs {}/{}'.format(i+1,epochs))\n",
    "    model.fit_generator(gen,epochs=1,steps_per_epoch=steps,validation_data=val_gen, validation_steps=val_steps)\n",
    "    model.save('./mw/epochs'+str(i+1)+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training for rest 20 epochs as lower learning rate but larger batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ie8x6Q6T5jjL"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(0.0001))\n",
    "model.load_weights('model_weights/epochs30.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwJgNksY0nvf"
   },
   "outputs": [],
   "source": [
    "batch_size=6\n",
    "steps=len(train_description)//batch_size\n",
    "val_steps=len(val_description)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745
    },
    "id": "38rpkA4Vnce3",
    "outputId": "cf59259b-cab9-4e24-a5a9-f787f6aa3373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 31/50\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 2.6700 - val_loss: 3.9165\n",
      "epochs 32/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.6109 - val_loss: 3.9159\n",
      "epochs 33/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.5894 - val_loss: 3.9159\n",
      "epochs 34/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.5770 - val_loss: 3.9198\n",
      "epochs 35/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.5680 - val_loss: 3.9238\n",
      "epochs 36/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.5590 - val_loss: 3.9286\n",
      "epochs 37/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.5513 - val_loss: 3.9299\n",
      "epochs 38/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.5438 - val_loss: 3.9351\n",
      "epochs 39/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.5373 - val_loss: 3.9361\n",
      "epochs 40/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.5364 - val_loss: 3.9424\n",
      "epochs 41/50\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 2.5286 - val_loss: 3.9471\n",
      "epochs 42/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.5181 - val_loss: 3.9512\n",
      "epochs 43/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.5156 - val_loss: 3.9492\n",
      "epochs 44/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.5121 - val_loss: 3.9536\n",
      "epochs 45/50\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 2.5099 - val_loss: 3.9621\n",
      "epochs 46/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.5043 - val_loss: 3.9607\n",
      "epochs 47/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.4981 - val_loss: 3.9622\n",
      "epochs 48/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.4987 - val_loss: 3.9713\n",
      "epochs 49/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.4946 - val_loss: 3.9666\n",
      "epochs 50/50\n",
      "1000/1000 [==============================] - 21s 21ms/step - loss: 2.4936 - val_loss: 3.9703\n"
     ]
    }
   ],
   "source": [
    "for i in range(30,50):\n",
    "    gen=data_generator(train_description,word_to_index,encoding_train,batch_size,max_len)\n",
    "    val_gen=data_generator(val_description,word_to_index,encoding_val,batch_size,max_len)\n",
    "    print('epochs {}/{}'.format(i+1,50))\n",
    "    model.fit_generator(gen,epochs=1,steps_per_epoch=steps,validation_data=val_gen, validation_steps=val_steps)\n",
    "    model.save('./mw/epochs'+str(i+1)+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "_c8dR8UE0N7j",
    "outputId": "6ed6e3ef-4e48-4cb1-d803-664a27030e9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102973440/102967424 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = ResNet50(weights='imagenet')\n",
    "rn = Model(inputs=base_model.input, outputs=base_model.get_layer('avg_pool').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laixkQy20--E"
   },
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    x = np.expand_dims(img, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    preds = rn.predict(x)\n",
    "    return preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eB25J6iBtjO_"
   },
   "outputs": [],
   "source": [
    "def caption(photo):\n",
    "    start = 'startseq'\n",
    "    for i in range(max_len):\n",
    "        sequence = [word_to_index[w] for w in start.split() if w in word_to_index]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_len)\n",
    "        y_pred = model.predict([photo,sequence], verbose=0)\n",
    "        y_pred = np.argmax(y_pred)\n",
    "        word = index_to_word[y_pred]\n",
    "        start += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    sents = start.split()\n",
    "    sents = sents[1:-1]\n",
    "    sents = ' '.join(sents)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fz0SgNmd4CHH"
   },
   "outputs": [],
   "source": [
    "def gen_caption(path):\n",
    "    img = image.load_img(path,target_size=(224,224))\n",
    "    img= image.img_to_array(img,dtype='uint8')\n",
    "    photo=preprocess(img)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(caption(photo.reshape((1,-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgpGnZVd3zaQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MODEL_GENERATOR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
